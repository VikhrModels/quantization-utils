# Quantization Utils - Bare Metal Setup

A comprehensive toolkit for quantizing large language models to GGUF format with support for multiple acceleration backends (CUDA, Metal, CPU).

## ğŸš€ Features

| Feature             | Status | Description                                       |
| ------------------- | ------ | ------------------------------------------------- |
| ğŸ–¥ï¸ **Bare Metal**    | âœ…      | Native installation without Docker                |
| ğŸ”§ **Auto Setup**    | âœ…      | Automatic environment detection and configuration |
| ğŸ¯ **Multi-Backend** | âœ…      | CUDA, Metal (Apple Silicon), and CPU support      |
| ğŸ“¦ **Conda Ready**   | âœ…      | Complete conda environment with all dependencies  |
| âš¡ **Quick Scripts** | âœ…      | Convenient scripts for common tasks               |
| ğŸ“Š **Perplexity**    | âœ…      | Automated quality testing of quantized models     |
| ğŸ” **Validation**    | âœ…      | Environment health checks and troubleshooting     |

## ğŸ“‹ Prerequisites

| Requirement | Minimum Version | Notes                     |
| ----------- | --------------- | ------------------------- |
| **Conda**   | Latest          | Miniconda or Anaconda     |
| **Python**  | 3.11+           | Installed via conda       |
| **Git**     | 2.0+            | For repository operations |
| **CMake**   | 3.14+           | For building llama.cpp    |

### GPU Support (Optional)

| Platform          | Requirements     | Acceleration               |
| ----------------- | ---------------- | -------------------------- |
| **NVIDIA**        | CUDA 11.8+       | âœ… CUDA acceleration        |
| **Apple Silicon** | macOS + M1/M2/M3 | âœ… Metal acceleration       |
| **Others**        | Any CPU          | âœ… Optimized CPU processing |

## ğŸ› ï¸ Quick Setup

### Option 1: Automated Setup (Recommended)

```bash
# Clone the repository
git clone https://github.com/Vikhrmodels/quantization-utils.git
cd quantization-utils

# Run the automated setup script
chmod +x scripts/setup.sh
./scripts/setup.sh
```

### Option 2: Manual Setup

```bash
# Create conda environment
conda env create -f environment.yml

# Activate environment
conda activate quantization-utils

# Run setup to install llama.cpp and prepare directories
python setup.py

# Add to PATH (if needed)
export PATH="$HOME/.local/bin:$PATH"
```

## ğŸ” Validation

Verify your installation:

```bash
# Check environment health
./scripts/validate.sh

# Quick test
conda activate quantization-utils
cd GGUF
python -c "from shared import validate_environment; validate_environment()"
```

## ğŸ“Š Usage Examples

### Basic Model Quantization

```bash
# Activate environment
conda activate quantization-utils

# Quantize a model with default settings
./scripts/quantize.sh microsoft/DialoGPT-medium

# Custom quantization levels
./scripts/quantize.sh Vikhrmodels/Vikhr-Gemma-2B-instruct -q Q4_K_M,Q5_K_M,Q8_0

# Force re-quantization
./scripts/quantize.sh microsoft/DialoGPT-medium --force
```

### Advanced Pipeline Usage

```bash
cd GGUF

# Full pipeline with all quantization levels
python pipeline.py --model_id microsoft/DialoGPT-medium

# Specific quantization levels only
python pipeline.py --model_id microsoft/DialoGPT-medium -q Q4_K_M -q Q8_0

# With perplexity testing
python pipeline.py --model_id microsoft/DialoGPT-medium --perplexity

# For gated models (requires HF token)
python pipeline.py --model_id meta-llama/Llama-2-7b-hf --hf_token $HF_TOKEN
```

### Perplexity Testing

```bash
# Test all quantized versions
./scripts/perplexity.sh microsoft/DialoGPT-medium

# Force recalculation
./scripts/perplexity.sh microsoft/DialoGPT-medium --force
```

## ğŸ“ Directory Structure

```
quantization-utils/
â”œâ”€â”€ ğŸ“„ environment.yml          # Conda environment definition
â”œâ”€â”€ ğŸ setup.py                 # Environment setup script
â”œâ”€â”€ ğŸ“– README.md                # This file
â”‚
â”œâ”€â”€ ğŸ”§ scripts/                 # Convenience scripts
â”‚   â”œâ”€â”€ setup.sh               # Automated setup
â”‚   â”œâ”€â”€ validate.sh             # Environment validation
â”‚   â”œâ”€â”€ quantize.sh             # Quick quantization
â”‚   â””â”€â”€ perplexity.sh           # Perplexity testing
â”‚
â””â”€â”€ ğŸ“¦ GGUF/                    # Main processing directory
    â”œâ”€â”€ ğŸ pipeline.py          # Main pipeline script
    â”œâ”€â”€ ğŸ shared.py            # Shared utilities
    â”œâ”€â”€ ğŸ“ models/              # Downloaded models
    â”œâ”€â”€ ğŸ“ imatrix/             # Importance matrices
    â”œâ”€â”€ ğŸ“ output/              # Final quantized models
    â”œâ”€â”€ ğŸ“ resources/           # Calibration data
    â”‚   â””â”€â”€ standard_cal_data/
    â””â”€â”€ ğŸ“ modules/             # Processing modules
        â”œâ”€â”€ convert.py
        â”œâ”€â”€ quantize.py
        â”œâ”€â”€ imatrix.py
        â””â”€â”€ perplexity.py
```

## âš™ï¸ Configuration Options

### Environment Variables

| Variable               | Description           | Example  |
| ---------------------- | --------------------- | -------- |
| `HF_TOKEN`             | HuggingFace API token | `hf_...` |
| `CUDA_VISIBLE_DEVICES` | GPU selection         | `0,1`    |
| `OMP_NUM_THREADS`      | CPU threads           | `8`      |

### Pipeline Parameters

| Parameter      | Description          | Default    |
| -------------- | -------------------- | ---------- |
| `--model_id`   | HuggingFace model ID | Required   |
| `--quants`     | Quantization levels  | All levels |
| `--force`      | Force reprocessing   | False      |
| `--perplexity` | Run quality tests    | False      |
| `--threads`    | Processing threads   | CPU count  |

### Quantization Levels

| Level    | Description        | Size     | Quality       |
| -------- | ------------------ | -------- | ------------- |
| `Q2_K`   | 2-bit quantization | Smallest | Good          |
| `Q4_K_M` | 4-bit mixed        | Balanced | Very Good     |
| `Q5_K_M` | 5-bit mixed        | Larger   | Excellent     |
| `Q6_K`   | 6-bit              | Large    | Near Original |
| `Q8_0`   | 8-bit              | Largest  | Original      |

## ğŸ› Troubleshooting

### Common Issues

| Issue                       | Solution                               |
| --------------------------- | -------------------------------------- |
| `conda: command not found`  | Install Miniconda/Anaconda             |
| `llama-quantize: not found` | Run `python setup.py`                  |
| `CUDA out of memory`        | Reduce batch size or use CPU           |
| `Permission denied`         | Check file permissions with `chmod +x` |

### Environment Problems

```bash
# Reset environment
conda env remove -n quantization-utils
conda env create -f environment.yml

# Reinstall llama.cpp
rm -rf ~/.local/bin/llama-*
python setup.py

# Check installation
./scripts/validate.sh
```

### Binary Issues

```bash
# Manual llama.cpp installation
cd /tmp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=$HOME/.local
make -j$(nproc)
make install
```

## ğŸ”§ Development

### Adding New Quantization Methods

1. Update `shared.py` with new `Quant` enum values
2. Modify `modules/quantize.py` to handle new methods
3. Update pipeline default quantization list
4. Test with validation scripts

### Custom Calibration Data

```python
# Add to GGUF/resources/standard_cal_data/
# Files should be UTF-8 text with one sample per line
```

## ğŸ“ˆ Performance Tips

| Tip              | Description                                       |
| ---------------- | ------------------------------------------------- |
| ğŸš€ **GPU Usage**  | Use CUDA/Metal for 5-10x speedup                  |
| ğŸ’¾ **Memory**     | Monitor RAM usage with large models               |
| ğŸ”„ **Batch Size** | Adjust based on available memory                  |
| ğŸ“Š **Threads**    | Set to CPU core count for optimal CPU performance |

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Test with `./scripts/validate.sh`
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the terms specified in the LICENSE file.

## ğŸ”— Links

- **llama.cpp**: https://github.com/ggerganov/llama.cpp
- **HuggingFace**: https://huggingface.co/
- **GGUF Format**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md

---

<div align="center">

**Ready to quantize? Start with `./scripts/setup.sh`** ğŸš€

</div>