# CUDA Dockerfile for GGUF quantization
# Optimized for NVIDIA GPU acceleration

FROM nvcr.io/nvidia/pytorch:25.05-py3

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Build arguments
ARG CUDA_ARCHITECTURES="52;61;70;75;80;86;89;90"
ARG THREADS=8

# Install additional system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    pkg-config \
    libopenblas-dev \
    jq \
    unzip \
    && rm -rf /var/lib/apt/lists/*

# Create working directory
WORKDIR /app

# Function to detect architecture and download latest llama.cpp CUDA release
RUN set -e && \
    ARCH=$(uname -m) && \
    case $ARCH in \
        x86_64) LLAMA_ARCH="x64" ;; \
        aarch64) LLAMA_ARCH="arm64" ;; \
        *) echo "Unsupported architecture: $ARCH" && exit 1 ;; \
    esac && \
    echo "Detected architecture: $ARCH -> llama.cpp: $LLAMA_ARCH" && \
    LATEST_RELEASE=$(curl -s https://api.github.com/repos/ggml-org/llama.cpp/releases/latest | jq -r .tag_name) && \
    echo "Latest llama.cpp release: $LATEST_RELEASE" && \
    DOWNLOAD_URL="https://github.com/ggml-org/llama.cpp/releases/download/$LATEST_RELEASE/llama-$LATEST_RELEASE-bin-ubuntu-$LLAMA_ARCH.zip" && \
    echo "Downloading from: $DOWNLOAD_URL" && \
    curl -L -o llama.cpp.zip "$DOWNLOAD_URL" && \
    unzip llama.cpp.zip && \
    rm llama.cpp.zip && \
    CURRENT_DIR=$(pwd) && \
    echo "Current directory: $CURRENT_DIR" && \
    for dir in llama-*-bin-ubuntu-*; do \
        if [ -d "$dir" ]; then \
            EXTRACTED_DIR="$dir" && \
            break; \
        fi; \
    done && \
    echo "Extracted directory: $EXTRACTED_DIR" && \
    chmod -R +x "$EXTRACTED_DIR"/bin/* 2>/dev/null || true && \
    chmod -R +x "$EXTRACTED_DIR"/lib/*.so* 2>/dev/null || true && \
    cp -r "$EXTRACTED_DIR"/lib/* /usr/local/lib/ 2>/dev/null || true && \
    ldconfig || \
    echo "Pre-built binaries not available, will build from source when needed"

# Add llama.cpp binaries to PATH
ENV PATH="/opt/llama.cpp/bin:$PATH"

# Copy requirements from parent directory and install Python dependencies
COPY requirements.txt requirements.txt
RUN pip3 install --no-cache-dir --break-system-packages -r requirements.txt

# Copy the GGUF directory
COPY . ./GGUF/

# Set working directory to GGUF
WORKDIR /app/GGUF

# Set build environment for CUDA
ENV CMAKE_ARGS="-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=${CUDA_ARCHITECTURES}"
ENV LLAMA_CPP_CMAKE_ARGS="-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=${CUDA_ARCHITECTURES}"
ENV CUDA_VISIBLE_DEVICES=0
ENV THREADS=${THREADS}

# Create necessary directories
RUN mkdir -p models imatrix wikitext-2-raw

# Default command
CMD ["/bin/bash"] 